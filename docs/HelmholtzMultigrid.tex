
\documentclass[]{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amsfonts} 
\usepackage[backend=bibtex,sorting=none]{biblatex}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
%\usepackage[]{biblatex}
\usepackage{parskip}
\setlength{\parindent}{15pt}
\addbibresource{ref.bib}

\begin{document}

\title{Multigrid Methods for Solving the "Good" Helmholtz Equation}
\author{D. Finn} 
%\institute{University at Buffalo, SUNY}
\date{\today}

\maketitle


\section{Introduction}

How do you solve a linear system of equations,
\begin{equation}
A x = b
\end{equation}
where $b \in \mathbb{R}^n$ is a known vector, $A \in \mathbb{R}^{n \times n}$ is a real, symmetric, positive definite matrix,  and is very large and sparse?  The traditional methods for solving this linear system can be extremely costly and typically converge slowly, if at all.  For example, given a matrix $A$ of size $n \times n$, Gaussian elimination has a complexity of $\mathcal{O} (n^3)$ which is not efficient when dealing with the large matrices that arise in numerical analysis \cite{Ispen1997}.  A major step forward came in the 1950s when researchers realized that classical iteration methods all lead to solutions sequences that span the \textit{Krylov Subspace}.  Based on this idea, new fast iterative methods, such as Conjugate Gradients and Generalized Minimum Residual (GMRES), were derived.  Then in the 1960s and 1970s, Nikolai Bakhvalov \cite{Bakhvalov1966}, Radii Petrovich Fedorenko \cite{Fedorenko1962, Fedorenko1964}, Achi Brandt \cite{Achi1973, Achi1977}, and Wolfgang Hackbusch \cite{Hackbusch1977}, developed \textit{Multigrid Methods} that further sped up iterative methods by applying them to multiple levels of discretizations.  

For this project, we will explore using multigrid methods through the Portable, Extensible Toolkit for Scientific Computation (PETSc) to solve the "Good" Helmholtz equation.  It is of interest to use the developed code to test and compare the multigrid solvers on CPUs and GPUs.  GPUs present an interesting potential for code speedup in certain circumstances, however some literature \cite{May2016} suggests that the GPU optimization will only become more efficient than CPU code at very large problem sizes.   We hope to explore this idea more with this work.

\section{Methods}

Classical numerical methods for solving linear systems have revolved around various implementations of Gaussian elimination.  These implementations work to exploit the sparse structure of the coefficient matrix but often still have computational complexities near $\mathcal{O} (n^3)$ or convergence rates too slow to be used in a reasonable amount of time.  Alternatively, numerous iterative methods have been developed that can significantly reduce computational complexity and speedup convergence. 

In this section, we explore in more depth some of the concepts of iterative solvers that are used in this project.  We start with Krylov and Multigrid methods and how PETSc handles both solvers.  Lastly we describe the Helmholtz formulation used in this project.

\subsection{Krylov Subspace Methods}

Starting with some initial guess $x_0$, Krylov methods bootstraps up to more accurate approximations, $x_k$ \cite{Ispen1997}.  In a given iteration, $k$,  Krylov methods produce an approximate solution, $x_k$, from a Krylov space generated by vector $c$,
\begin{equation}
\mathcal{K}_k (A, c) = span \lbrace c, Ac, ..., A^{k-1} c \rbrace.
\end{equation}

\subsubsection{Conjugate Gradients}

To understand Krylov Methods further, we look at a specific Krylov algorithm, \textit{Conjugate Gradients (CG)}.  CG works by generating successive approximations (Krylov subspaces) to the solution, residuals corresponding to the approximations, and search directions  used in updating the approximations and residuals.  Figure \ref{CGAlg} contains a \textit{preconditioned conjugate gradients algorithm} where $K$ is a preconditioner and $z_i$ is the preconditioned residual. To obtain the \textit{unpreconditioned conjugate gradients algorithm}, the preconditioner $K$ can be set to the identity matrix $I$.
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.6]{/Users/danny/Documents/UBuffalo/Semester_8_2021/PHY506_CompPhys2/Final_Project/Midsemester_Update/CG_Algorithm.png}
\end{center}
\caption{The conjugate gradients algorithm \cite{Henk2000}.}
\label{CGAlg}
\end{figure}
In more detail, starting with some initial guess $x_0$,  the first residual is calculated using, 
\begin{equation}
r_0 = b - A x_0.
\end{equation}
In each successive CG iteration, the iterates $x_i$ are updated by a multiple $\alpha_i$ of the search direction $p_i$, 
\begin{equation}
x_i = x_{i-1} + \alpha_i p_i,
\end{equation}
and the residual is updated with the equation,
\begin{align}
r_i = r_{i-1} &- \alpha_i q_i \;\; where, \\
q_i &= A p_i.
\end{align}
The constant,
\begin{equation}
\alpha_i = \frac{r_{i-1}^T r_{i-1}}{p_i^T q_i} 
\end{equation}
is chosen to minimize $r_i^T A^{-1} r_i$ over all possible $\alpha_i$.  The search directions $p_i$ are updated using the residuals,
\begin{equation}
p_i = r_i + \beta_{i-1} p_{i-1},
\end{equation}
where,
\begin{equation}
\beta_i = \frac{r_{i}^T r_{i}}{r_{i-1}^T r_{i-1}},
\end{equation}
is chose to ensure $p_i$ and $Ap_{i-1}$ are orthogonal.  Equivalently, this implies any two successive residuals, $r_{i-1}$ and $r_i$, are also orthogonal.  The process of orthogonalizing the search directions and residuals is called \textit{Gram-Schmidt Process} \cite{Keener2000}.  Furthermore, it can be shown that the choice of $\beta_i$ makes the search direction $p_i$ and the residual $r_i$ orthogonal to all previous iterations. 

One main draw of the CG method is that, while the size of the Krylov subspaces can be quite large, the number of vectors that are actually kept in the memory is small.  Additionally the time complexity for CG, given a finite element formulation of a second-order elliptic boundary value problem, is $\mathcal{O}^{3/2}$ in 2D and $\mathcal{O}^{4/3}$ in 3D \cite{Shewchuk1994}. Recalling that the comlexity for Guassian Elimination is $\mathcal{O}^{3}$, this is a considerable improvement in efficiency.

\subsection{Multigrid Methods}

Since their first description in the 60s, multigrid methods have become well known for being the fastest numerical method for solving elliptic boundary value problems.  A good derivation and explanation of multigrid methods is given in \cite{Briggs2000}, but we will provide some of the basic methodologies here. The basic concept behind multigrid is to provide a fine grid solver with a good initial guess obtained from a (or a sequence of) coarse grid solve(s) \cite{Briggs2000}.  Relaxation (or iterative) methods are typically represented by Jacobi or Gauss-Seidel iteration.  To start, consider the simple Two-Grid Correction Scheme ($v^h \leftarrow MG(v^h, f^h)$):
\begin{itemize}
\item Relax $\nu_1$ times on $A^h u^h = f^h$ on $Ω^h$ with initial guess $v_h$.
\item Compute the fine-grid residual $r^h = f^h - A^h v^h$ and restrict it to the coarse grid by $r^{2h} = I^{2h}_h r^h$.
\item Solve $A^{2h}e^{2h} = r^{2h}$ on $Ω^{2h}$.
\item Interpolate the coarse-grid error to the fine grid by $e^h = I^h_{2h} e^{2h}$ and correct the fine-grid approximation by $v^h \leftarrow v^h + e^h$.
\item Relax $\nu_2$ times on $A^h u^h = f^h$ on $\Omega^h$ with initial guess $v^h$.
\end{itemize}
Here, $\Omega^h$ is the finest discretization level and $\Omega^2h$ is the coarse grid, $u^h$ is the solution vector, $v^h$ is the current iterate approximate solution, $r^h$ is the residual, $e^h = u^h - v^h$ is the error and $I^h_{2h}$ is the interpolation operator that takes coarse-grid vectors and produces fine-grid vectors.  In practice $\nu_1$ is usually 1,2 or 3.  In the scheme, relaxation on the fine grid eliminates the oscillatory components of the error, leaving behind only the smooth error.  The smooth error leads to a very good interpolation on the fine grid meaning the correction should be effective. 

As described above, the two-grid correction scheme still leaves room for some improvements.  Primarily, a more direct solution of the residual equation is possible by applying recursion to the coarse-grid correction.  This leads to the more common multigrid scheme, the V-Cycle Scheme ($v^h \leftarrow V^h (v^h, f^h)$):
\begin{itemize}
\item Relax on $A^h u^h = f^h$ $\nu_1$ times with initial guess $v_h$.
\item Compute $f^{2h} = I^{2h}_h r^h$.
	\begin{itemize}
	\item[$\bullet$] Relax on $A^{2h} u^{2h} = f^{2h}$ $\nu_1$ times with initial guess $v^{2h} = 0$.
	\item[$\bullet$] Compute $f^{4h} = I^{4h}_{2h} r^{2h}$.
		\begin{itemize}
		\item[$\bullet$] Relax on $A^{4h} u^{4h} = f^{4h}$ $\nu_1$ times with initial guess $v^{4h} = 0$.
		\item[$\bullet$] Compute $f^{8h} = I^{8h}_{4h} r^{4h}$.
			\begin{itemize}
				\item[$\vdots$]
				\item Solve $A^{Lh} u^{Lh} = f^{Lh}$.
				\item[$\vdots$]
			\end{itemize}
		\item[$\bullet$] Correct $v^{4h} \leftarrow v^{4h} + I^{4h}_{8h} v^{8h}$.
		\item[$\bullet$] Relax on $A^{4h} u^{4h} = f^{4h}$ $\nu_2$ times with initial guess $v^{4h}$.
		\end{itemize}
	\item[$\bullet$] Correct $v^{2h} \leftarrow v^{2h} + I^{2h}_{4h} v^{4h}$.
	\item[$\bullet$] Relax on $A^{2h} u^{2h} = f^{2h}$ $\nu_2$ times with initial guess $v^{2h}$.
	\end{itemize}
\item Correct $v^{h} \leftarrow v^{h} + I^{h}_{2h} v^{2h}$.
\item Relax on $A^{h} u^{h} = f^{h}$ $\nu_2$ times with initial guess $v^{h}$.
\end{itemize}

The V-Cycle is just one of many different multigrid schemes.

\subsection{Helmholtz Equation}


Solving the general Helmholtz equation $-\Delta u +k^2 u = f$ with multigrid methods is an open problem in the field of numerical methods \cite{Ernst2012}.  For that reason, we consider only the case where $k^2=1$, referred to as the "Good" Helmholtz equation \cite{Farrell2018},
\begin{align}
-\Delta u + u &= f \;\; in \;\; \Omega, \\
u &= 0 \;\; on \;\; \Gamma_D, \\
\nabla u\cdot n &= g \;\; on \;\; \Gamma_N.
\end{align}
To discretize this equation, we use the finite elements method.


\printbibliography 


%
% All documents must end with this command.
%
\end{document}



